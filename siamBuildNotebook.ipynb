{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9311c-3e97-4724-857b-3261a109fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Libraries\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# %%\n",
    "!pip install -q keras\n",
    "!pip3 install torch torchvision\n",
    "!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n",
    "!pip install -q xgboost==0.4a30\n",
    "!apt-get -qq install -y graphviz && pip install -q pydot\n",
    "\n",
    "# %%\n",
    "!pip install tensorflow[and-cuda]\n",
    "\n",
    "# %%\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "# %%\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# %%\n",
    "import os\n",
    "\n",
    "# %%\n",
    "import os; os.system(\"xdg-open /media/hc17/I/SiameseNetwrok/aug1\")\n",
    "\n",
    "# %%\n",
    "!pip install Pillow\n",
    "\n",
    "\n",
    "# %%\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def read_images_from_folder(folder_path):\n",
    "    image_list = []\n",
    "    valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".gif\"}  # Add more if needed\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        _, extension = os.path.splitext(filename)\n",
    "\n",
    "        if extension.lower() in valid_extensions:\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    image_list.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    return image_list\n",
    "\n",
    "folder_path = \"/media/hc17/I/SiameseNetwrok/aug1\"\n",
    "image_list = read_images_from_folder(folder_path)\n",
    "\n",
    "# Now 'image_list' contains PIL Image objects of the images in the specified folder\n",
    "\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import uuid\n",
    "from PIL import Image\n",
    "\n",
    "def rename_images_with_uuid(folder_path):\n",
    "    valid_extensions = {\".jpg\", \".jpeg\"}\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        _, extension = os.path.splitext(filename)\n",
    "\n",
    "        if extension.lower() in valid_extensions:\n",
    "            try:\n",
    "                # Generate a unique name using UUID\n",
    "                unique_name = str(uuid.uuid4()) + extension\n",
    "                new_path = os.path.join(folder_path, unique_name)\n",
    "\n",
    "                # Rename the file\n",
    "                os.rename(file_path, new_path)\n",
    "\n",
    "                print(f\"Renamed {filename} to {unique_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error renaming {filename}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/media/hc17/I/SiameseNetwrok/aug1\"\n",
    "rename_images_with_uuid(folder_path)\n",
    "\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import uuid\n",
    "from PIL import Image\n",
    "\n",
    "def rename_images_with_uuid(folder_path):\n",
    "    valid_extensions = {\".jpg\", \".jpeg\"}\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        _, extension = os.path.splitext(filename)\n",
    "\n",
    "        if extension.lower() in valid_extensions:\n",
    "            try:\n",
    "                # Generate a unique name using UUID\n",
    "                unique_name = str(uuid.uuid4()) + extension\n",
    "                new_path = os.path.join(folder_path, unique_name)\n",
    "\n",
    "                # Rename the file\n",
    "                os.rename(file_path, new_path)\n",
    "\n",
    "                print(f\"Renamed {filename} to {unique_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error renaming {filename}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/media/hc17/I/SiameseNetwrok/aug2\"\n",
    "rename_images_with_uuid(folder_path)\n",
    "\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import uuid\n",
    "from PIL import Image\n",
    "\n",
    "def rename_images_with_uuid(folder_path):\n",
    "    valid_extensions = {\".jpg\", \".jpeg\"}\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        _, extension = os.path.splitext(filename)\n",
    "\n",
    "        if extension.lower() in valid_extensions:\n",
    "            try:\n",
    "                # Generate a unique name using UUID\n",
    "                unique_name = str(uuid.uuid4()) + extension\n",
    "                new_path = os.path.join(folder_path, unique_name)\n",
    "\n",
    "                # Rename the file\n",
    "                os.rename(file_path, new_path)\n",
    "\n",
    "                print(f\"Renamed {filename} to {unique_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error renaming {filename}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/media/hc17/I/SiameseNetwrok/aug3\"\n",
    "rename_images_with_uuid(folder_path)\n",
    "\n",
    "\n",
    "# %%\n",
    "# importing fucntional API's\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# %%\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# %%\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# %%\n",
    "current_directory\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def merge_folders(source_folder1, source_folder2, destination_folder):\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Iterate through the first source folder and copy images to the destination folder\n",
    "    for filename in os.listdir(source_folder1):\n",
    "        source_path = os.path.join(source_folder1, filename)\n",
    "        destination_path = os.path.join(destination_folder, filename)\n",
    "        shutil.copy2(source_path, destination_path)\n",
    "\n",
    "    # Iterate through the second source folder and copy images to the destination folder\n",
    "    for filename in os.listdir(source_folder2):\n",
    "        source_path = os.path.join(source_folder2, filename)\n",
    "        destination_path = os.path.join(destination_folder, filename)\n",
    "        shutil.copy2(source_path, destination_path)\n",
    "\n",
    "# Example usage:\n",
    "source_folder1 = '/media/hc17/I/SiameseNetwrok/aug2'\n",
    "source_folder2 = '/media/hc17/I/SiameseNetwrok/aug3'\n",
    "destination_folder = '/media/hc17/I/SiameseNetwrok/negative_images'\n",
    "\n",
    "merge_folders(source_folder1, source_folder2, destination_folder)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "positive_path= os.path.join('GaugesData' , 'posGauges')\n",
    "negative_path =  os.path.join('GaugesData' , 'negGauges')\n",
    "anchor_path = os.path.join('GaugesData','ancGauges')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "anchor = tf.data.Dataset.list_files(anchor_path + '/*.jpg').take(100)\n",
    "positive = tf.data.Dataset.list_files(positive_path + '/*.jpg').take(100)\n",
    "\n",
    "negative = tf.data.Dataset.list_files(negative_path + '/*.jpg').take(100)\n",
    "\n",
    "# %%\n",
    "test = positive.as_numpy_iterator()\n",
    "\n",
    "# %%\n",
    "test.next()\n",
    "\n",
    "# %%\n",
    "# preprocessing\n",
    "\n",
    "# %%\n",
    "def preprocess(filePath):\n",
    "    byteImage = tf.io.read_file(filePath)\n",
    "    img = tf.io.decode_jpeg(byteImage)\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    img = img/255.0\n",
    "    return img\n",
    "\n",
    "# %%\n",
    "trialPositiveImages = preprocess('GaugesData/posGauges/0e4a6860-e18f-477e-a3f3-fdc60bf21c59.jpg')\n",
    "\n",
    "# %%\n",
    "plt.imshow(trialPositiveImages)\n",
    "\n",
    "# %%\n",
    "# labelled data\n",
    "\n",
    "# %%\n",
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)### all data whether positive or negative gets concatenated\n",
    "\n",
    "# %%\n",
    "sampleData = data.as_numpy_iterator()\n",
    "\n",
    "# %%\n",
    "example = sampleData.next()\n",
    "\n",
    "# %%\n",
    "example\n",
    "\n",
    "# %%\n",
    "# train and test partition\n",
    "\n",
    "# %%\n",
    "def preprocess_twin(inputImage , validationImage, label):\n",
    "    return(preprocess(inputImage),preprocess(validationImage), label)\n",
    "\n",
    "# %%\n",
    "res = preprocess_twin(*example)\n",
    "# *example passed because\n",
    "\n",
    "# %%\n",
    "res\n",
    "\n",
    "# %%\n",
    "len(res)\n",
    "\n",
    "# %%\n",
    "res[0]\n",
    "\n",
    "# %%\n",
    "plt.imshow(res[0])\n",
    "\n",
    "# %%\n",
    "plt.imshow(res[1])\n",
    "\n",
    "# %%\n",
    "res[2]\n",
    "\n",
    "# %%\n",
    "# data loader pipeline?\n",
    "\n",
    "# %%\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size = 1024)\n",
    "\n",
    "# %%\n",
    "\n",
    "samps = data.as_numpy_iterator()\n",
    "\n",
    "# %%\n",
    "sample = samps.next()\n",
    "\n",
    "# %%\n",
    "plt.imshow(sample[0])\n",
    "\n",
    "# %%\n",
    "plt.imshow(sample[1])\n",
    "\n",
    "# %%\n",
    "sample[2]\n",
    "\n",
    "# %%\n",
    "len(data)\n",
    "\n",
    "# %%\n",
    "training_data = data.take(round(len(data) * .7))\n",
    "# passing data in the form of batches instead all at once\n",
    "training_data = training_data.batch(5)\n",
    "# prefetch the upcoming image so to avoid any kind of bottleneck in the NN\n",
    "training_data = training_data.prefetch(7)\n",
    "\n",
    "# %%\n",
    "train_samples = training_data.as_numpy_iterator()\n",
    "\n",
    "# %%\n",
    "train_samples = train_samples.next()\n",
    "\n",
    "# %%\n",
    "train_samples\n",
    "\n",
    "# %%\n",
    "len(train_samples)\n",
    "\n",
    "# %%\n",
    "len(train_samples[0])\n",
    "\n",
    "# %%\n",
    "#testing partitions\n",
    "\n",
    "# %%\n",
    "test_data = data.skip(round(len(data) * .7)) # do not take the 70 percent data\n",
    "test_data = test_data.take(round(len(data) * .3)) # take the remaining 30 percent data\n",
    "\n",
    "# %% [markdown]\n",
    "# Model Engineering\n",
    "\n",
    "# %%\n",
    "def makeEmbedding():\n",
    "    inp = Input(shape=(100,100,3), name = 'input_image')\n",
    "\n",
    "\n",
    "    #first block - \n",
    "        # convolution layer1\n",
    "        # max pooling layer 1\n",
    "    \n",
    "    convolutionLayer1 = Conv2D(64,(10,10), activation ='relu')(inp)  ## the code is pertaining to the reference research paper on one shot \n",
    "    maxPoolingLayer1 = MaxPooling2D(64,(2,2), padding='same')(convolutionLayer1)\n",
    "\n",
    "    # second block:\n",
    "        # convolution layer 2\n",
    "        # max pooling layer 2\n",
    "    convolutionLayer2 = Conv2D(128,(7,7), activation='relu')(maxPoolingLayer1)\n",
    "    maxPoolingLayer2 = MaxPooling2D(64,(2,2), padding='same')(convolutionLayer2)\n",
    "\n",
    "    # Third block\n",
    "        # convolution layer 3\n",
    "        # convolution layer 3\n",
    "    convolutionLayer3 = Conv2D(128,(4,4), activation='relu')(maxPoolingLayer2)\n",
    "    maxPoolingLayer3 = MaxPooling2D(64,(2,2), padding='same')(convolutionLayer3)\n",
    "\n",
    "    # Final Block\n",
    "        # Convolution Layer 4\n",
    "        # Flattening to produce Feature Vector\n",
    "\n",
    "    convolutionLayer4 = Conv2D(246,(4,4), activation= 'relu')(maxPoolingLayer3)\n",
    "    flattenLayer1 = Flatten()(convolutionLayer4)\n",
    "    #dense layer\n",
    "    denseLayer1 = Dense(4096, activation='sigmoid')(flattenLayer1)\n",
    "\n",
    "    return Model(inputs=[inp],outputs=[denseLayer1],name='embedding')\n",
    "\n",
    "# %%\n",
    "embedding = makeEmbedding()\n",
    "\n",
    "# %%\n",
    "embedding.summary()\n",
    "\n",
    "# %%\n",
    "# Distance Layer Build\n",
    "\n",
    "# %%\n",
    "# Siamese L1 Distance class\n",
    "class L1Dist(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "       \n",
    "    # Magic happens here - similarity calculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)\n",
    "\n",
    "# %% [markdown]\n",
    "# Siamese Model\n",
    "\n",
    "# %%\n",
    "def makeSiameseModel():\n",
    "    # anchor image in the network \n",
    "    input_image = Input(name = 'input_img' , shape=(100,100,3))\n",
    "\n",
    "    # validation image in the network\n",
    "    validation_image = Input(name = 'validation_img' , shape=(100,100,3))\n",
    "\n",
    "    #Combine Siamese Distance Components:\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image),embedding(validation_image))\n",
    "\n",
    "    # classification layer\n",
    "    classifier = Dense(1,activation='sigmoid')(distances)\n",
    "\n",
    "    return Model(inputs=[input_image, validation_image] , outputs = classifier , name = 'Siamese-Network')\n",
    "\n",
    "# %%\n",
    "siamese_model = makeSiameseModel()\n",
    "\n",
    "# %%\n",
    "siamese_model.summary()\n",
    "\n",
    "# %%\n",
    "# Data training\n",
    "\n",
    "# %%\n",
    "binaryCrossLoss = tf.losses.BinaryCrossentropy()\n",
    "\n",
    "# %%\n",
    "opt = tf.keras.optimizers.Adam(1e-4) # 0.0001\n",
    "\n",
    "# %%\n",
    "\n",
    "checkpoint_dir = './training_checkpoint'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)\n",
    "\n",
    "# %%\n",
    "@tf.function\n",
    "# @tf allows us to compile a function into a callable Tensor flow graph\n",
    "def train_step(batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        #Get anchor and pos/neg images:\n",
    "        X = batch[:2]\n",
    "        # Get labels for the images\n",
    "        Y = batch[2]\n",
    "\n",
    "        # Forward pass:\n",
    "        Yhat = siamese_model(X, training = True)\n",
    "\n",
    "        # Calculate Loss:\n",
    "        loss = binaryCrossLoss(Y , Yhat)\n",
    "\n",
    "\n",
    "    #Calculate gradients:\n",
    "    grad = tape.gradient(loss , siamese_model.trainable_variables)\n",
    "\n",
    "    # calculate updated weights and apply them to the model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# %%\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "\n",
    "# %%\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "\n",
    "def train(data, EPOCHS):\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Creating metric objects \n",
    "        r = Recall()\n",
    "        p = Precision()\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            loss = train_step(batch)\n",
    "            yhat = siamese_model.predict(batch[:2])\n",
    "            r.update_state(batch[2], yhat)\n",
    "            p.update_state(batch[2], yhat) \n",
    "            progbar.update(idx+1)\n",
    "        print(loss.numpy(), r.result().numpy(), p.result().numpy())\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "\n",
    "# %%\n",
    "EPOCHS = 50\n",
    "\n",
    "# %%\n",
    "train(training_data, EPOCHS)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
